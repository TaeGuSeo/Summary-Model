{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcFNouU47iH6WfgPNJOyac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaeGuSeo/Terms-and-Conditions-Analysis-System/blob/main/%EC%9C%A0%EB%B6%88%EB%A6%AC_%ED%8C%90%EB%8B%A8_%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 유불리 판단 모델\n",
        "\n",
        "# 입력받은 약관 전문을 문자열로 나눈 뒤, 문장별로 사용자에게 유리한지 불리한지를 판단하여 유리조항리스트와 불리조항리스트로 추출하는 과정에 필요한 모델.\n",
        "# 초기 일렉트라, 버트, 코버트를 사용해보았지만 학습 과정에서의 손실과 정확도를 비교해보았을 때, 버트쪽 모델이 사용하기에 적합할 것으로 보여 버트, 코버트를 사용하여 수행"
      ],
      "metadata": {
        "id": "B4wQYI5nO-AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7ou2WVGMW5p"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import re\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, set_seed\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import os\n",
        "import nltk\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import copy\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
        "\n",
        "# 유불리 모델 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "model_path = '.\\model_state_dict.pkl'\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# 요약 모델 초기화\n",
        "nltk.download('punkt')\n",
        "tokenizer_summ = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
        "model_summ = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
        "model_summ.eval()\n",
        "'''\n",
        "\n",
        "# 요약 모델 초기화\n",
        "nltk.download('punkt')\n",
        "tokenizer_summ = AutoTokenizer.from_pretrained('Youngwoo9/T5_Pyeongsan')\n",
        "model_summ = AutoModelForSeq2SeqLM.from_pretrained('Youngwoo9/T5_Pyeongsan')\n",
        "model_summ.eval()\n",
        "\n",
        "# 요약 리스트 초기화\n",
        "ad_sentences = []\n",
        "disad_sentences = []\n",
        "ad_sum_sentences = []\n",
        "disad_sum_sentences = []\n",
        "\n",
        "# 제목, 키워드 리스트 초기화\n",
        "title = ['미지정']\n",
        "keywords =['미지정1', '미지정2', '미지정3']\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/api/upload', methods=['POST'])\n",
        "def handle_image_upload():\n",
        "    if 'image' not in request.files:\n",
        "        return jsonify({'error': '이미지를 제공하지 않았습니다.'}), 400\n",
        "    image = request.files['image']\n",
        "    image = cv2.imdecode(np.frombuffer(image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    text = pytesseract.image_to_string(gray_image, lang='kor')\n",
        "\n",
        "\n",
        "    print('추출된 텍스트:', text)\n",
        "    response_data = {'text': text}\n",
        "\n",
        "    return jsonify(response_data)\n",
        "\n",
        "\n",
        "@app.route('/api/process', methods=['POST'])\n",
        "def process_text():\n",
        "    data = request.json\n",
        "    sentences = data.get('sentences', [])\n",
        "    results = copy.deepcopy(sentences)\n",
        "\n",
        "    # 리스트 초기화\n",
        "    ad_sentences.clear()\n",
        "    disad_sentences.clear()\n",
        "    ad_sum_sentences.clear()\n",
        "    disad_sum_sentences.clear()\n",
        "\n",
        "    # input data 전처리\n",
        "    sentences_input = [\"[CLS] \" + str(s) + \" [SEP]\" for s in sentences]\n",
        "\n",
        "\n",
        "    class ClassificationCollator(object):\n",
        "        # 토크나이저, 라벨 인코더, 최대 시퀀스 길이 설정 & 객체 초기화\n",
        "        def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
        "            self.use_tokenizer = use_tokenizer\n",
        "            self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "            self.labels_encoder = labels_encoder\n",
        "            return\n",
        "\n",
        "        # 설정된 토크나이저, 라벨 인코더, 최대 시퀀스 길이를 이용해 데이터를 전처리\n",
        "        def __call__(self, sequences):\n",
        "            texts = sequences\n",
        "            inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "\n",
        "            return inputs\n",
        "\n",
        "    Classificaiton_collator = ClassificationCollator(use_tokenizer=tokenizer,\n",
        "                                                          labels_encoder=2,\n",
        "                                                          max_sequence_len=128)\n",
        "\n",
        "    print('Dealing with Data...')\n",
        "    sentences_input = DataLoader(sentences_input, collate_fn=Classificaiton_collator)\n",
        "    print('Created `sentences` with %d batches!'%len(sentences_input))\n",
        "\n",
        "\n",
        "    def predict_sentiment(tensor, model, tokenizer):\n",
        "        inputs = tensor\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prob_1, prob_2 = probabilities[0]\n",
        "\n",
        "        return prob_1.item(), prob_2.item()\n",
        "\n",
        "\n",
        "    for tensor, text in zip(sentences_input, sentences) :\n",
        "        print('들어온 문장 : ',text,'\\n')\n",
        "        # import pdb; pdb.set_trace()\n",
        "        print()\n",
        "        prob_1, prob_2 = predict_sentiment(tensor, model, tokenizer)\n",
        "        print('유리확률 : ',prob_1,'\\n')\n",
        "        print()\n",
        "        print('불리확률 : ',prob_2,'\\n')\n",
        "        print()\n",
        "        if prob_1 > 0.995:\n",
        "            ad_sentences.append(text)\n",
        "        if prob_2 > 0.995:\n",
        "            disad_sentences.append(text)\n",
        "\n",
        "    print(ad_sentences)\n",
        "    print(disad_sentences)\n",
        "\n",
        "    prefix = \"summarize: \"\n",
        "\n",
        "    for sentence in ad_sentences:\n",
        "        if not sentence.endswith('.'):\n",
        "            ad_sum_sentences.append(sentence)\n",
        "        else:\n",
        "            input_text = prefix + sentence\n",
        "\n",
        "            inputs = tokenizer_summ(input_text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "            output = model_summ.generate(**inputs, num_beams=3, do_sample=True, min_length=1, max_length=len(sentence))\n",
        "            decoded_output = tokenizer_summ.batch_decode(output, skip_special_tokens=True)[0]\n",
        "            decoded_output = decoded_output.strip()\n",
        "\n",
        "            if decoded_output:\n",
        "                sentences_summ = nltk.sent_tokenize(decoded_output)\n",
        "            if sentences_summ:\n",
        "                result = sentences_summ[0]\n",
        "                ad_sum_sentences.append(result)\n",
        "\n",
        "    for sentence in disad_sentences:\n",
        "        if not sentence.endswith('.'):\n",
        "            disad_sum_sentences.append(sentence)\n",
        "        else:\n",
        "            input_text = prefix + sentence\n",
        "\n",
        "            inputs = tokenizer_summ(input_text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "            output = model_summ.generate(**inputs, num_beams=3, do_sample=True, min_length=1, max_length=len(sentence))\n",
        "            decoded_output = tokenizer_summ.batch_decode(output, skip_special_tokens=True)[0]\n",
        "            decoded_output = decoded_output.strip()\n",
        "            if decoded_output:\n",
        "                sentences_summ = nltk.sent_tokenize(decoded_output)\n",
        "            if sentences_summ:\n",
        "                result = sentences_summ[0]\n",
        "                disad_sum_sentences.append(result)\n",
        "\n",
        "    print(ad_sum_sentences)\n",
        "    print(disad_sum_sentences)\n",
        "\n",
        "    return jsonify({'text': results})\n",
        "\n",
        "\n",
        "@app.route('/process', methods=['GET'])\n",
        "def process():\n",
        "    response_data = {\n",
        "        'positive_texts': ad_sentences,\n",
        "        'negative_texts': disad_sentences,\n",
        "        'sum_p_texts': ad_sum_sentences,\n",
        "        'sum_n_texts': disad_sum_sentences\n",
        "        }\n",
        "    return jsonify(response_data)\n",
        "\n",
        "\n",
        "@app.route('/title', methods=['GET'])\n",
        "def title():\n",
        "    title_texts = [\"001_온라인게임_가공\"]\n",
        "    response_data = {\n",
        "            'title_texts': title_texts\n",
        "        }\n",
        "\n",
        "    return jsonify(response_data)\n",
        "\n",
        "\n",
        "@app.route('/keyword', methods=['GET'])\n",
        "def keyword():\n",
        "    keyword_texts = [\"001_온라인게임_가공\"]\n",
        "\n",
        "    response_data = {\n",
        "            'keyword_texts': keyword_texts\n",
        "        }\n",
        "\n",
        "    return jsonify(response_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run('0.0.0.0', port=5000, threaded=True)"
      ]
    }
  ]
}