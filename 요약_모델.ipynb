{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn+V2mPwSbpBRRBiFmIVHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaeGuSeo/Summary-Model/blob/main/%EC%9A%94%EC%95%BD_%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kYzTdgOLFmF"
      },
      "outputs": [],
      "source": [
        "###'paust/pko-t5-base'기반 pre-trained 모델에 약관 원문/요약 데이터셋을 이용해 fine-tuning을 진행한 모델"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 모델 초기화\n",
        "nltk.download('punkt')\n",
        "tokenizer_summ = AutoTokenizer.from_pretrained('Youngwoo9/T5_Pyeongsan')\n",
        "model_summ = AutoModelForSeq2SeqLM.from_pretrained('Youngwoo9/T5_Pyeongsan')\n",
        "model_summ.eval()"
      ],
      "metadata": {
        "id": "fzT8iP2gLHMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in ad_sentences:\n",
        "  if not sentence.endswith('.'):\n",
        "    ad_sum_sentences.append(sentence)\n",
        "  else:\n",
        "    input_text = prefix + sentence.replace('\"', '')\n",
        "\n",
        "    inputs = tokenizer_summ(input_text, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "    output = model_summ.generate(**inputs, num_beams=3, do_sample=True, min_length=1, max_length=len(sentence))\n",
        "    decoded_output = tokenizer_summ.batch_decode(output, skip_special_tokens=True)[0]\n",
        "    decoded_output = decoded_output.strip()\n",
        "\n",
        "    if decoded_output:\n",
        "      sentences_summ = nltk.sent_tokenize(decoded_output)\n",
        "      if sentences_summ:\n",
        "        result = sentences_summ[0]\n",
        "        ad_sum_sentences.append(result)\n",
        "\n",
        "for sentence in disad_sentences:\n",
        "  if not sentence.endswith('.'):\n",
        "    disad_sum_sentences.append(sentence)\n",
        "  else:\n",
        "    input_text = prefix + sentence.replace('\"', '')\n",
        "\n",
        "    inputs = tokenizer_summ(input_text, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "    output = model_summ.generate(**inputs, num_beams=3, do_sample=True, min_length=1, max_length=len(sentence))\n",
        "    decoded_output = tokenizer_summ.batch_decode(output, skip_special_tokens=True)[0]\n",
        "    decoded_output = decoded_output.strip()\n",
        "\n",
        "    if decoded_output:\n",
        "      sentences_summ = nltk.sent_tokenize(decoded_output)\n",
        "      if sentences_summ:\n",
        "        result = sentences_summ[0]\n",
        "        disad_sum_sentences.append(result)"
      ],
      "metadata": {
        "id": "mP44lceALHJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}